{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep RL Lab Session_full.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ROKQVmp7YGm",
        "colab_type": "text"
      },
      "source": [
        "Install Dependencies\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "dv15uwsW7YGn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bce8caf8-64f2-46fa-ecf1-27411fe399ee"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNDdtwwA7YGt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23bbd674-fa45-401d-ff64-d6aff9979597"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrm1-ffu7YG1",
        "colab_type": "text"
      },
      "source": [
        "Import Dependencies\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7EXs2vb7YG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "import random\n",
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Sci_23M7YG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import time\n",
        "import scipy.sparse.csgraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOZ0VKKP7YG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zszvLEAQ7YHC",
        "colab_type": "text"
      },
      "source": [
        "## Taxi Gym Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26xEZLZW7YHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "72f5ce33-c808-4aea-8565-af71e760cd99"
      },
      "source": [
        "env = gym.make(\"Taxi-v2\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PSrfagM7YHJ",
        "colab_type": "text"
      },
      "source": [
        "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
        "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inNxqB3g7YHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b615dabc-15ac-49e9-dac3-37bd786e8241"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBjmSyR-7YHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "13b873e4-25bd-4301-ad84-865aa7a9ba43"
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9VdS9C7YHc",
        "colab_type": "text"
      },
      "source": [
        "When the taxi contains the passenger, passenger index is 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jZ8ofMU7YHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "64b458e5-8428-4d2a-9860-ebb15ea706b1"
      },
      "source": [
        "state = env.encode(3, 1, 4, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 336\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[42m_\u001b[0m: | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAkplF_I7YHg",
        "colab_type": "text"
      },
      "source": [
        "### Solve an episode yourself: \n",
        "Use the following actions:\n",
        "- 0 = south\n",
        "- 1 = north\n",
        "- 2 = east\n",
        "- 3 = west\n",
        "- 4 = pickup\n",
        "- 5 = dropoff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGM3zG6a7YHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5caa1ba2-a506-4728-ee00-e07cc63da11e"
      },
      "source": [
        "env.s = 328\n",
        "env.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqBDs4f-7YHq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ca013ee6-39b4-4060-c451-d95c0923f77a"
      },
      "source": [
        "# Use Ctrl+Enter to take an action\n",
        "env.step(1)\n",
        "env.render()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_NBIFiH7YHt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "fcb081a0-8302-4535-d489-ed0094fa9f06"
      },
      "source": [
        "env.step(3)\n",
        "env.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXzDCd1tBO2k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6dafdcb6-2ef0-45d4-af4a-118f1bfd11b0"
      },
      "source": [
        "env.step(0)\n",
        "env.render()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYc6dh6KBQUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "22fc0fbf-a4fd-4a2c-cf87-d8def31f06ab"
      },
      "source": [
        "env.step(5)\n",
        "env.render()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0AQ6MPb7YHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f2694cce-f9ce-45c3-cf4f-44e30a007521"
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 256\n",
            "Penalties incurred: 58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnREzJnL7YH0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "22aa88fd-312b-434f-c8de-dff42c2732d1"
      },
      "source": [
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'].getvalue())\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 256\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piGWiRC57YH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtll31An7YH_",
        "colab_type": "text"
      },
      "source": [
        "# Q-Learning\n",
        "Our environment is deterministic, so all equations presented here are\n",
        "also formulated deterministically for the sake of simplicity. In the\n",
        "reinforcement learning literature, they would also contain expectations\n",
        "over stochastic transitions in the environment.\n",
        "\n",
        "Our aim will be to train a policy that tries to maximize the discounted,\n",
        "cumulative reward\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
        "$R_{t_0}$ is also known as the *return*. The discount,\n",
        "$\\gamma$, should be a constant between $0$ and $1$\n",
        "that ensures the sum converges. It makes rewards from the uncertain far\n",
        "future less important for our agent than the ones in the near future\n",
        "that it can be fairly confident about.\n",
        "\n",
        "The main idea behind Q-learning is that if we had a function\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
        "us what our return would be, if we were to take an action in a given\n",
        "state, then we could easily construct a policy that maximizes our\n",
        "rewards:\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "However, we don't know everything about the world, so we don't have\n",
        "access to $Q^*$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qQXnfBC7YIF",
        "colab_type": "text"
      },
      "source": [
        "# Tabular Q-Learning\n",
        "\n",
        "We use a table to represent $Q*$. Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
        "\n",
        "$Q (s_t, a_t)←(1−α)*Q(s_t,a_t) + α*(r_t + γ*max_a Q(s_{t+1}, a))$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $α$ (alpha) is the learning rate ($0<α≤1$) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n",
        "\n",
        "- $γ$ (gamma) is the discount factor ($0≤γ≤1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnwfKeK-7YIG",
        "colab_type": "text"
      },
      "source": [
        "Tabular Q-Learning steps\n",
        "- Initialize the Q-table by all zeros.\n",
        "- Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
        "- Travel to the next state (S') as a result of that action (a).\n",
        "- For all possible actions from the state (S') select the one with the highest Q-value.\n",
        "- Update Q-table values using the equation.\n",
        "- Set the next state as the current state.\n",
        "- If goal state is reached, then end and repeat the process.\n",
        "\n",
        "Exploiting learned values:\n",
        "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
        "\n",
        "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called $ϵ$ \"epsilon\" to cater to this during training.\n",
        "\n",
        "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIB-dNWb7YIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft3GnIfN7YIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "ff2e8442-7638-4522-999f-42e8cee1b92c"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "max_episode_length = 500\n",
        "num_training_episodes = 1000\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, num_training_episodes+1):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    for _ in range(max_episode_length):\n",
        "        \n",
        "        ### Write code to sample action\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            ## Explore\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            ## Exploit\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        reward += get_shaped_reward(env.decode(prev_state.numpy()), env.decode(state)) # after adding reward shaping, the performance is still not good. Because it needs to share new info in each iteration stage\n",
        "        # need to reach out of the states and bootstrap\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        ### Write Q-Learning update equations\n",
        "        old_value = q_table[state, action]\n",
        "        new_value = (1 - alpha) * old_value + alpha*(reward + gamma * np.max(q_table[next_state]))\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 1000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 6.5 s, sys: 41 ms, total: 6.54 s\n",
            "Wall time: 6.51 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dstxJFop7YIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22f6216f-6998-4179-9e37-4c26eb89cc3e"
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.32350668, -2.32069885, -2.32405342, -2.32270488, -5.71311619,\n",
              "       -6.33258433])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEP_8OwQ7YIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "739e4632-932d-4c40-e3c3-2771c9599b43"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "max_episode_length = 500\n",
        "total_epochs, total_penalties, total_reward = 0., 0., 0.\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    eps_reward = 0.\n",
        "    \n",
        "    for _ in range(max_episode_length):\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "        \n",
        "        eps_reward += reward\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "        \n",
        "    \n",
        "    total_reward += eps_reward\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average reward per episode: {total_reward / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 348.09\n",
            "Average reward per episode: -341.58\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv81xklmLJWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# small action space, works well; but when actions are more, there are more info needed to be shared among the states. So we want to use deep-Q learning\n",
        "# policy and off-policy, policy gradients could help. For unpolicy, give examples and then estimate easily"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W976qKRH7YIb",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Learning\n",
        "Now, we train a neural network to resemble $Q^*$.\n",
        "\n",
        "For our training update rule, we'll use a fact that every $Q$\n",
        "function for some policy obeys the Bellman equation:\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "The difference between the two sides of the equality is known as the\n",
        "temporal difference error, $\\delta$:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n",
        "\n",
        "To minimise this error, we will use the [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss).\n",
        "The Huber loss acts\n",
        "like the mean squared error when the error is small, but like the mean\n",
        "absolute error when the error is large - this makes it more robust to\n",
        "outliers when the estimates of $Q$ are very noisy. We calculate\n",
        "this over a batch of transitions, $B$, sampled from the replay\n",
        "memory:\n",
        "\n",
        "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
        "\n",
        "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
        "   \\end{cases}\\end{align}\n",
        "\n",
        "\n",
        "#### Q-network\n",
        "Our model will be a neural network that takes in the state as input and outputs the Q-value for each action in that state. In effect, the network is trying to predict the *expected return* of\n",
        "taking each action given the current input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X6LgeeC7YIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT6O7uik7YIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, 64) # first layer, convert into 64 vector\n",
        "        self.linear1 = nn.Linear(64, 64) # another layer\n",
        "        self.linear2 = nn.Linear(64, output_size) # last output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.embedding(x))\n",
        "        x = nn.ReLU()(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VS5uazw7YIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(env.action_space.n)]], dtype=torch.long)\n",
        "\n",
        "\n",
        "def plot(stat, averaging_size):\n",
        "    durations_t = torch.tensor(stat, dtype=torch.float)\n",
        "    if len(durations_t) >= averaging_size:\n",
        "        means = durations_t.unfold(0, averaging_size, 1).mean(1).view(-1)\n",
        "        plt.figure(2)\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Duration')\n",
        "        plt.ylim(-10, 510)\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyActHw37YIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), dtype=torch.uint8)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch)\n",
        "    state_action_values = state_action_values.gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    next_state_values = next_state_values.detach()\n",
        "    \n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values*GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_crj_z057YIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "5b616f6a-d247-44e4-bb85-b7637b40727a"
      },
      "source": [
        "num_episodes = 100\n",
        "max_episode_length = 500\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 5000\n",
        "TARGET_UPDATE = 10000\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "policy_net = DQN(env.observation_space.n, env.action_space.n)\n",
        "target_net = DQN(env.observation_space.n, env.action_space.n)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n",
        "episode_durations = []\n",
        "\n",
        "losses = deque(maxlen=10000)\n",
        "eps_rewards = deque(maxlen=100)\n",
        "successes = deque(maxlen=100)\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    state = env.reset()\n",
        "    state = torch.tensor([state])\n",
        "    eps_reward = 0\n",
        "    for t in range(max_episode_length):\n",
        "        # Select and perform an action\n",
        "        prev_state = state\n",
        "        action = select_action(state)\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "#         reward += get_shaped_reward(env.decode(prev_state.numpy()), env.decode(state))\n",
        "\n",
        "        #Reward Normalization\n",
        "        reward /= 20.0 \n",
        "        \n",
        "        state = torch.tensor([state])\n",
        "        eps_reward += reward\n",
        "        reward = torch.tensor([reward]).float()\n",
        "\n",
        "        if not done:\n",
        "            next_state = state \n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(prev_state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        loss = optimize_model()\n",
        "        if loss is not None:\n",
        "            losses.append(loss)\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    if reward > 0.75:\n",
        "        successes.append(1.)\n",
        "    else:\n",
        "        successes.append(0.)\n",
        "    \n",
        "    eps_rewards.append(eps_reward)\n",
        "    clear_output(wait=True)\n",
        "    print(\"Episode: {}, Mean Reward: {:.3f}, Mean Loss: {:.3f}, Success Rate: {:.3f} ({:d})\".format(\n",
        "            i_episode, np.mean(eps_rewards), np.mean(losses), np.mean(successes), len(successes)))\n",
        "    \n",
        "    episode_durations.append(t + 1)\n",
        "    plot(episode_durations, 10)\n",
        "    time.sleep(0.0001)\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 99, Mean Reward: -35.670, Mean Loss: 0.000, Success Rate: 0.000 (100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-HBp-KLHDov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the model will not perform well, it needs much much more data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBYx8FP17YI4",
        "colab_type": "text"
      },
      "source": [
        "## Reward Shaping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0TfcWHE7YI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the shortest path\n",
        "\n",
        "def compute_full_distances_fast(grid):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        `grid`: maze plan of shape (m, n)\n",
        "    Output:\n",
        "        `distances`: distances between each position in the grid (m, n, m, n)\n",
        "    \"\"\"\n",
        "    # compute valid positions\n",
        "    m, n = grid.shape\n",
        "    distances = np.zeros((m, n, m, n), dtype=np.int32) + m * n\n",
        "    valid = [(i, j) for i in range(m) for j in range(n) if grid[i, j] == 0]\n",
        "\n",
        "    # initialize distances: 0 to self, 1 to adjacent squares\n",
        "    for i1, j1 in valid:\n",
        "        distances[i1, j1, i1, j1] = 0  # distance to self\n",
        "        for i2, j2 in [(i1 - 1, j1), (i1 + 1, j1), (i1, j1 - 1), (i1, j1 + 1)]:\n",
        "            if 0 <= i2 < m and 0 <= j2 < n and grid[i2, j2] == 0:\n",
        "                distances[i1, j1, i2, j2] = 1  # distance to neighbors\n",
        "\n",
        "    distances = scipy.sparse.csgraph.floyd_warshall(distances.reshape(m * n, m * n), directed=False)\n",
        "    distances = distances.reshape((m, n, m, n)).astype(np.int32)\n",
        "\n",
        "    return distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUVpf9Zi7YI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid = np.zeros((11, 11))\n",
        "grid[0,:] = 1\n",
        "grid[-1,:] = 1\n",
        "grid[:,0] = 1\n",
        "grid[:,-1] = 1\n",
        "grid[6:,2] = 1\n",
        "grid[6:,6] = 1\n",
        "grid[:3,4] = 1\n",
        "\n",
        "distances = compute_full_distances_fast(grid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPNHAnypH05S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "db030c7c-db19-4a04-ad87-338db434626f"
      },
      "source": [
        "plt.imshow(grid)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa624a94278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACr5JREFUeJzt3V2InQedx/Hvbyctmrr4kgbZvLDJ\nRakEYVsZSrUgS9OldRW7F8vSQsWVhdysWkWQuje93QsRvRAh1LqCpWWJhS1SjOILy8KSdZoGbBLF\nUmuTJt2OkVXxJm3878UcdychaYZ5nvMS/98PlM6cnJ7nx6Tfec45c3KSqkJSL38y7wGSZs/wpYYM\nX2rI8KWGDF9qyPClhgxfasjwpYYMX2poyywPduM7lmrP7utmeUiplRdPvcYvf3UhV7veTMPfs/s6\n/uvw7lkeUmrltrtPbeh63tWXGjJ8qSHDlxoyfKkhw5caGhR+knuS/DTJ80keGmuUpOnadPhJloAv\nAx8A9gH3J9k31jBJ0zPkjH8b8HxVvVBV54EngHvHmSVpmoaEvxNY/2qB05PLLpLkQJKVJCur5y4M\nOJyksUz9yb2qOlhVy1W1vH3b0rQPJ2kDhoT/MrD+9be7JpdJWnBDwv8RcFOSvUmuB+4DnhpnlqRp\n2vQf0qmq15N8HDgMLAGPVtXx0ZZJmppBfzqvqp4Gnh5pi6QZ8ZV7UkOGLzVk+FJDhi81NNO33hrb\n3TtumfeEmTl85tiot9fpa7foxv693QjP+FJDhi81ZPhSQ4YvNWT4UkOGLzVk+FJDhi81ZPhSQ4Yv\nNWT4UkOGLzVk+FJDhi81ZPhSQ4YvNWT4UkOGLzVk+FJDhi81ZPhSQ4YvNWT4UkOGLzVk+FJDmw4/\nye4kP0hyIsnxJA+OOUzS9Az5K7ReBz5TVUeT/CnwTJLvVtWJkbZJmpJNn/Gr6mxVHZ18/FvgJLBz\nrGGSpmeUx/hJ9gC3AkfGuD1J0zU4/CRvAb4JfKqqfnOZXz+QZCXJyuq5C0MPJ2kEg8JPch1r0T9W\nVU9e7jpVdbCqlqtqefu2pSGHkzSSIc/qB/gqcLKqvjDeJEnTNuSMfwfwEeDOJMcm//z1SLskTdGm\nf5xXVf8BZMQtkmbEV+5JDRm+1JDhSw0ZvtTQkNfqa4bu3nHLvCfoj4hnfKkhw5caMnypIcOXGjJ8\nqSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnyp\nIcOXGjJ8qSHDlxoyfKkhw5caGhx+kqUkzyb51hiDJE3fGGf8B4GTI9yOpBkZFH6SXcAHgUfGmSNp\nFoae8b8IfBb4/ZWukORAkpUkK6vnLgw8nKQxbDr8JB8CXq2qZ97oelV1sKqWq2p5+7alzR5O0oiG\nnPHvAD6c5EXgCeDOJN8YZZWkqdp0+FX1uaraVVV7gPuA71fVA6MtkzQ1/hxfamjLGDdSVT8EfjjG\nbUmaPs/4UkOGLzVk+FJDhi81ZPhSQ4YvNWT4UkOGLzVk+FJDhi81ZPhSQ4YvNWT4UkOGLzVk+FJD\nhi81ZPhSQ4YvNWT4UkOGLzVk+FJDhi81ZPhSQ4YvNTTKX6jxx+LwmWOj3dbdO24Z7bauBWN+7aDf\n12/WPONLDRm+1JDhSw0ZvtSQ4UsNDQo/yduSHErykyQnk7x3rGGSpmfoj/O+BHy7qv42yfXA1hE2\nSZqyTYef5K3A+4G/B6iq88D5cWZJmqYhd/X3AqvA15I8m+SRJDeMtEvSFA0JfwvwHuArVXUr8Dvg\noUuvlORAkpUkK6vnLgw4nKSxDAn/NHC6qo5MPj/E2jeCi1TVwaparqrl7duWBhxO0lg2HX5VvQKc\nSnLz5KL9wIlRVkmaqqHP6n8CeGzyjP4LwMeGT5I0bYPCr6pjwPJIWyTNiK/ckxoyfKkhw5caMnyp\nIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoyfKkh\nw5caMnypIcOXGjJ8qSHDlxoyfKkhw5caMnypIcOXGjJ8qSHDlxoaFH6STyc5nuS5JI8nedNYwyRN\nz6bDT7IT+CSwXFXvBpaA+8YaJml6ht7V3wK8OckWYCtwZvgkSdO26fCr6mXg88BLwFng11X1nUuv\nl+RAkpUkK6vnLmx+qaTRDLmr/3bgXmAvsAO4IckDl16vqg5W1XJVLW/ftrT5pZJGM+Su/l3Az6tq\ntapeA54E3jfOLEnTNCT8l4Dbk2xNEmA/cHKcWZKmachj/CPAIeAo8OPJbR0caZekKdoy5D+uqoeB\nh0faImlGfOWe1JDhSw0ZvtSQ4UsNGb7UkOFLDRm+1JDhSw0ZvtSQ4UsNGb7UkOFLDRm+1JDhSw0Z\nvtSQ4UsNGb7UkOFLDRm+1JDhSw0ZvtSQ4UsNGb7UkOFLDQ36CzXm7fCZY/OecEWLvO1a4Ndvujzj\nSw0ZvtSQ4UsNGb7UkOFLDV01/CSPJnk1yXPrLntHku8m+dnk32+f7kxJY9rIGf9fgHsuuewh4HtV\ndRPwvcnnkq4RVw2/qv4d+NUlF98LfH3y8deBvxl5l6Qp2uxj/HdW1dnJx68A7xxpj6QZGPzkXlUV\nUFf69SQHkqwkWVk9d2Ho4SSNYLPh/3eSPwOY/PvVK12xqg5W1XJVLW/ftrTJw0ka02bDfwr46OTj\njwL/Ns4cSbOwkR/nPQ78J3BzktNJ/gH4Z+CvkvwMuGvyuaRrxFX/dF5V3X+FX9o/8hZJM+Ir96SG\nDF9qyPClhgxfasjwpYay9sK7GR0sWQV+sYGr3gj8cspzNmuRt8Fi71vkbbDY+za67c+ravvVrjTT\n8DcqyUpVLc97x+Us8jZY7H2LvA0We9/Y27yrLzVk+FJDixr+wXkPeAOLvA0We98ib4PF3jfqtoV8\njC9puhb1jC9pihYq/CT3JPlpkueTLNT7+CXZneQHSU4kOZ7kwXlvulSSpSTPJvnWvLdcKsnbkhxK\n8pMkJ5O8d96b/iDJpye/p88leTzJm+a8Z+pvcLsw4SdZAr4MfADYB9yfZN98V13kdeAzVbUPuB34\nxwXbB/AgcHLeI67gS8C3q+pdwF+wIDuT7AQ+CSxX1buBJeC++a6a/hvcLkz4wG3A81X1QlWdB55g\n7U09F0JVna2qo5OPf8va/7g757vq/yXZBXwQeGTeWy6V5K3A+4GvAlTV+ar6n/muusgW4M1JtgBb\ngTPzHDOLN7hdpPB3AqfWfX6aBQprvSR7gFuBI/NdcpEvAp8Ffj/vIZexF1gFvjZ5KPJIkhvmPQqg\nql4GPg+8BJwFfl1V35nvqssa9Q1uFyn8a0KStwDfBD5VVb+Z9x6AJB8CXq2qZ+a95Qq2AO8BvlJV\ntwK/Y0H+LobJY+V7WfvmtAO4IckD8131xq72BrcbsUjhvwzsXvf5rsllCyPJdaxF/1hVPTnvPevc\nAXw4yYusPUS6M8k35jvpIqeB01X1h3tIh1j7RrAI7gJ+XlWrVfUa8CTwvjlvupwNv8HtRixS+D8C\nbkqyN8n1rD3B8tScN/2fJGHtMerJqvrCvPesV1Wfq6pdVbWHta/b96tqYc5aVfUKcCrJzZOL9gMn\n5jhpvZeA25Nsnfwe72dBnni8xKhvcHvV99yblap6PcnHgcOsPbP6aFUdn/Os9e4APgL8OMmxyWX/\nVFVPz3HTteQTwGOTb+ovAB+b8x4AqupIkkPAUdZ+cvMsc34F3+QNbv8SuDHJaeBh1t7Q9l8nb3b7\nC+DvBh3DV+5J/SzSXX1JM2L4UkOGLzVk+FJDhi81ZPhSQ4YvNWT4UkP/C6+TeoVey2s4AAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sANLe8cIV5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "distances[distances > 40] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBmtBnyXIEuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9f0c9d5a-6780-40be-b93f-085d0d33381d"
      },
      "source": [
        "plt.imshow(distances[3,2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa62459a1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nw5caMnypIcOXGvofgjXIhntQuH0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46lFz0IQ7YJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shaped_reward(prev_state, state):\n",
        "    if prev_state is None:\n",
        "        return 0\n",
        "    s = []\n",
        "    for i in prev_state:\n",
        "        s.append(i)\n",
        "    x1, y1 = s[0], s[1]\n",
        "    if s[2] == 4:\n",
        "        goal = s[3]\n",
        "    else:\n",
        "        goal = s[2]\n",
        "        \n",
        "    ps = []\n",
        "    for i in state:\n",
        "        ps.append(i)\n",
        "    x2, y2 = ps[0], ps[1]\n",
        "    \n",
        "    if goal == 0:\n",
        "        gx, gy = 0, 0\n",
        "    elif goal == 1:\n",
        "        gx, gy = 0, 4\n",
        "    elif goal == 2:\n",
        "        gx, gy = 4, 0\n",
        "    elif goal == 3:\n",
        "        gx, gy = 4, 3\n",
        "    \n",
        "    gx = 2*gx + 1\n",
        "    gy = 2*gy + 1\n",
        "    x1 = 2*x1 + 1\n",
        "    x2 = 2*x2 + 1\n",
        "    y1 = 2*y1 + 1\n",
        "    y2 = 2*y2 + 1\n",
        "    \n",
        "    prev_dist = distances[gx,gy][x1,y1]\n",
        "    dist = distances[gx,gy][x2,y2]\n",
        "    \n",
        "    if prev_dist > dist:\n",
        "        return 6.\n",
        "    else:\n",
        "        return -4."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Co4_v1T7YJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0d58da55-a819-41a6-aaa5-f9e6b4321888"
      },
      "source": [
        "num_episodes = 100\n",
        "max_episode_length = 500\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 1.0\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 5000\n",
        "TARGET_UPDATE = 10000\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "policy_net = DQN(env.observation_space.n, env.action_space.n)\n",
        "target_net = DQN(env.observation_space.n, env.action_space.n)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n",
        "episode_durations = []\n",
        "\n",
        "losses = deque(maxlen=10000)\n",
        "eps_rewards = deque(maxlen=100)\n",
        "successes = deque(maxlen=100)\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    state = env.reset()\n",
        "    state = torch.tensor([state])\n",
        "    eps_reward = 0\n",
        "    for t in range(max_episode_length):\n",
        "        # Select and perform an action\n",
        "        prev_state = state\n",
        "        action = select_action(state)\n",
        "        state, reward, done, _ = env.step(action.item())\n",
        "        reward += get_shaped_reward(env.decode(prev_state.numpy()), env.decode(state))\n",
        "        reward /= 20.0\n",
        "        state = torch.tensor([state])\n",
        "        eps_reward += reward\n",
        "        reward = torch.tensor([reward]).float()\n",
        "\n",
        "        if not done:\n",
        "            next_state = state \n",
        "        else:\n",
        "            next_state = None\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(prev_state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        loss = optimize_model()\n",
        "        if loss is not None:\n",
        "            losses.append(loss)\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "    if reward > 0.75:\n",
        "        successes.append(1.)\n",
        "    else:\n",
        "        successes.append(0.)\n",
        "    \n",
        "    eps_rewards.append(eps_reward)\n",
        "    clear_output(wait=True)\n",
        "    print(\"Episode: {}, Mean Reward: {:.3f}, Mean Loss: {:.3f}, Success Rate: {:.3f} ({:d})\".format(\n",
        "            i_episode, np.mean(eps_rewards), np.mean(losses), np.mean(successes), len(successes)))\n",
        "    \n",
        "    episode_durations.append(t + 1)\n",
        "    plot(episode_durations, 10)\n",
        "    time.sleep(0.0001)\n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 99, Mean Reward: -12.076, Mean Loss: 0.003, Success Rate: 0.970 (100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA24uWT4UljRiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qTMiUtT7YJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}